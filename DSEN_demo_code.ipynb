{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baking-joyce",
   "metadata": {},
   "source": [
    "# DSEN Training Pipeline Step-by-Step Breakdown\n",
    "\n",
    "This notebook provides a detailed explanation of the DSEN training pipeline code. We will cover:\n",
    "- Environment setup and library imports\n",
    "- Utility function definitions\n",
    "- Evaluation and training functions\n",
    "- Argument parsing and directory setup\n",
    "- Data preparation and model initialization\n",
    "- The training loop and testing phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-hybrid",
   "metadata": {},
   "source": [
    "## Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable CUDA DSA for torch (if needed)\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "\n",
    "# Standard libraries\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Data manipulation and scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# PyTorch and related utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Additional libraries for metrics and distances\n",
    "from scipy.stats import pearsonr, entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn import metrics\n",
    "\n",
    "# Custom modules (make sure these are in your working directory or PYTHONPATH)\n",
    "from models import DSEN\n",
    "from utils import Data_utility\n",
    "from Optim import Optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-recall",
   "metadata": {},
   "source": [
    "## Utility Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path, dic_name):\n",
    "    \"\"\"\n",
    "    Create a directory if it does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(path, dic_name)\n",
    "    if os.path.exists(path):\n",
    "        print(\"----Dic existed----\")\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "        print(\"----Dic created successfully----\")\n",
    "    return path\n",
    "\n",
    "def find_xy_flow(flow_df, TCMA_cbg):\n",
    "    \"\"\"\n",
    "    Assign latitude and longitude information to the flow dataframe based on TCMA_cbg.\n",
    "    \"\"\"\n",
    "    TCMA_cbg_center = TCMA_cbg.copy()\n",
    "    flow_df['start_lat'] = None\n",
    "    flow_df['start_lon'] = None\n",
    "    flow_df['end_lat'] = None\n",
    "    flow_df['end_lon'] = None\n",
    "\n",
    "    for index, row in tqdm(TCMA_cbg_center.iterrows()):\n",
    "        start_lat = row['y']\n",
    "        start_lon = row['x']\n",
    "        end_lat = row['y']\n",
    "        end_lon = row['x']\n",
    "        flow_df.loc[flow_df.o_cbg == row['CensusBlockGroup'], 'start_lat'] = start_lat\n",
    "        flow_df.loc[flow_df.o_cbg == row['CensusBlockGroup'], 'start_lon'] = start_lon\n",
    "        flow_df.loc[flow_df.d_cbg == row['CensusBlockGroup'], 'end_lat'] = end_lat\n",
    "        flow_df.loc[flow_df.d_cbg == row['CensusBlockGroup'], 'end_lon'] = end_lon\n",
    "\n",
    "    return flow_df\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Jensen-Shannon Divergence between two distributions.\n",
    "    \"\"\"\n",
    "    p = np.array(p) / np.sum(p)\n",
    "    q = np.array(q) / np.sum(q)\n",
    "    return jensenshannon(p, q) ** 2\n",
    "\n",
    "def common_part_of_commuters(generated_flow_flat, real_flow_flat, numerator_only=False):\n",
    "    \"\"\"\n",
    "    Compute the Common Part of Commuters (CPC) metric.\n",
    "    \"\"\"\n",
    "    nonzero_indices = np.nonzero(real_flow_flat)\n",
    "    filtered_real_flow = real_flow_flat[nonzero_indices]\n",
    "    filtered_generated_flow = generated_flow_flat[nonzero_indices]\n",
    "\n",
    "    filtered_real_flow = np.maximum(filtered_real_flow, 0)\n",
    "    numerator = 2 * np.sum(np.minimum(filtered_real_flow, filtered_generated_flow))\n",
    "    denominator = np.sum(filtered_real_flow + filtered_generated_flow)\n",
    "    cpc = numerator / denominator if denominator != 0 else 0.0\n",
    "    return cpc\n",
    "\n",
    "def NRMSE(generated_flow, real_flow):\n",
    "    \"\"\"\n",
    "    Compute the Normalized Root Mean Squared Error (NRMSE) for each sample.\n",
    "    \"\"\"\n",
    "    NRMSE_val = 0.0\n",
    "    for i in range(len(generated_flow)):\n",
    "        select_indice = np.where(real_flow[i] != 0)[0]\n",
    "        predictions = generated_flow[i][select_indice].copy()\n",
    "        actuals = real_flow[i][select_indice].copy()\n",
    "        mse = np.mean((predictions - actuals) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        norm_factor = np.max(actuals) - np.min(actuals)\n",
    "        if norm_factor != 0:\n",
    "            NRMSE_val += rmse / norm_factor\n",
    "    return NRMSE_val / len(generated_flow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-virginia",
   "metadata": {},
   "source": [
    "## Evaluation and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, X, Y, model, test_save_path, batch_size=64, mode=\"valid\"):\n",
    "    \"\"\"\n",
    "    Evaluate or test the model on given data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_flow = []\n",
    "    real_flow = []\n",
    "\n",
    "    for x, y in data.get_batches(X, Y, batch_size):\n",
    "        x = x.clone().detach().cuda()\n",
    "        y = y.clone().detach().cuda()\n",
    "        out, attention_weights_snap1, attention_weights_snap2, evo_embedding = model(data, x)\n",
    "        generated_flow.append(out.cpu().detach().numpy())\n",
    "        real_flow.append(y.cpu().numpy())\n",
    "\n",
    "    generated_flow = np.concatenate(generated_flow)\n",
    "    real_flow = np.concatenate(real_flow)\n",
    "\n",
    "    cpc = common_part_of_commuters(generated_flow, real_flow)\n",
    "    corr, _ = pearsonr(generated_flow, real_flow)\n",
    "    nrmse = np.sqrt(np.mean((generated_flow - real_flow) ** 2))\n",
    "    mape = np.mean(np.abs((real_flow - generated_flow) / real_flow)) * 100\n",
    "    js = js_divergence(generated_flow, real_flow)\n",
    "\n",
    "    if mode == \"test\":\n",
    "        o_index_list = X[:, 0]\n",
    "        d_index_list = X[:, 1]\n",
    "        test_results = pd.DataFrame.from_dict({\n",
    "            'o': o_index_list, \n",
    "            'd': d_index_list, \n",
    "            'generated_flow': generated_flow, \n",
    "            'Actual_flow': real_flow\n",
    "        })\n",
    "        test_results.to_csv(test_save_path + '12345/results/generated_flow_test_set.csv', index=False)\n",
    "\n",
    "    return cpc, corr, nrmse, mape, js\n",
    "\n",
    "def train(data, X, Y, model, criterion_mse, lambda_reg, optim, epoch, batch_size):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    generated_flow = []\n",
    "    real_flow = []\n",
    "    evo_embedding = []\n",
    "\n",
    "    last_attention_weights_snap1 = None\n",
    "    last_attention_weights_snap2 = None\n",
    "    last_edge_index_snap1 = None\n",
    "    last_edge_index_snap2 = None\n",
    "\n",
    "    for x, y in data.get_batches(X, Y, batch_size):\n",
    "        model.zero_grad()\n",
    "        x = x.clone().detach().cuda()\n",
    "        y = y.clone().detach().cuda()\n",
    "        out, attention_weights_snap1, attention_weights_snap2, Batch_evo_embedding = model(data, x)\n",
    "\n",
    "        edge_index_snap1, attention_weights_snap1 = attention_weights_snap1\n",
    "        edge_index_snap2, attention_weights_snap2 = attention_weights_snap2\n",
    "\n",
    "        last_attention_weights_snap1 = attention_weights_snap1.cpu().detach().numpy()\n",
    "        last_attention_weights_snap2 = attention_weights_snap2.cpu().detach().numpy()\n",
    "        last_edge_index_snap1 = edge_index_snap1.cpu().detach().numpy()\n",
    "        last_edge_index_snap2 = edge_index_snap2.cpu().detach().numpy()\n",
    "        Batch_evo_embedding = Batch_evo_embedding.cpu().detach().numpy()\n",
    "\n",
    "        batch_loss = criterion_mse(out, y)\n",
    "        l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "        batch_loss += lambda_reg * l2_norm\n",
    "\n",
    "        generated_flow.append(out.cpu().detach().numpy())\n",
    "        real_flow.append(y.cpu().numpy())\n",
    "        evo_embedding.append(Batch_evo_embedding)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        grad_norm = optim.step()\n",
    "        total_loss += batch_loss.cpu().data.numpy()\n",
    "\n",
    "    generated_flow = np.concatenate(generated_flow)\n",
    "    real_flow = np.concatenate(real_flow)\n",
    "    evo_embedding = np.concatenate(evo_embedding)\n",
    "\n",
    "    print(\"Generated Flow:\", generated_flow)\n",
    "    print(\"Real Flow:\", real_flow)\n",
    "\n",
    "    cpc = common_part_of_commuters(generated_flow, real_flow)\n",
    "    corr, _ = pearsonr(generated_flow, real_flow)\n",
    "    nrmse = np.sqrt(np.mean((generated_flow - real_flow) ** 2))\n",
    "    mape = np.mean(np.abs((real_flow - generated_flow) / real_flow)) * 100\n",
    "    js = js_divergence(generated_flow, real_flow)\n",
    "\n",
    "    return (total_loss, cpc, corr, nrmse, mape, js, \n",
    "            last_attention_weights_snap1, last_attention_weights_snap2, \n",
    "            last_edge_index_snap1, last_edge_index_snap2, evo_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-floating",
   "metadata": {},
   "source": [
    "## Argument Parsing and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument parsing for model configuration and hyperparameters\n",
    "parser = argparse.ArgumentParser(description='DSEN on Minneapolis')\n",
    "parser.add_argument('--model', type=str, default='DSEN', help='The model of DSEN')\n",
    "parser.add_argument('--epochs', type=int, default=1500, help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='batch size')\n",
    "parser.add_argument('--seed', type=int, default=12345, help='random seed')\n",
    "parser.add_argument('--gpu', type=int, default=0, help=\"The Index of GPU where we want to run the code\")\n",
    "parser.add_argument('--cuda', type=str, default=True)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--clip', type=float, default=10., help='gradient clipping')\n",
    "parser.add_argument('--optim', type=str, default='adam')\n",
    "parser.add_argument('--test_model_name', type=str, default='full_DSEN', help='the name of tested model')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up directories for results, logs, and model weights\n",
    "path = \"/\"\n",
    "path_result = make_dir(path, str(args.seed))\n",
    "path_log = make_dir(path_result, args.test_model_name + \"log\")\n",
    "path_model = make_dir(path_result, \"pkl\")\n",
    "path_att_weight = make_dir(path_result, \"att_weight\")\n",
    "path_evo_feature = make_dir(path_result, \"evo_feature\")\n",
    "path_result = make_dir(path_result, \"results\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=path_result + '_process.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(\"----Splitting the training and testing data----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-consumer",
   "metadata": {},
   "source": [
    "## Data Preparation and GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using a custom Data_utility class\n",
    "Data = Data_utility('42')  # '42' acts as a seed for splitting data\n",
    "\n",
    "# Configure GPU settings based on the arguments\n",
    "args.cuda = args.gpu is not None\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: CUDA device is available, consider using --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Optionally print full tensor information\n",
    "torch.set_printoptions(profile='full')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-depth",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----Building models----\")\n",
    "# Initialize the DSEN model with specified parameters\n",
    "model = eval(args.model).Model(\n",
    "    node_num_features=22, \n",
    "    flow_num_features=1, \n",
    "    head=1, \n",
    "    loc_embedding_dim=24, \n",
    "    evo_emb_dim=64,\n",
    "    evo_emb_hidden_dim=128, \n",
    "    flow_generator_hidden_dim=128, \n",
    "    dropout_p=0.3\n",
    ")\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "nParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('----Number of trainable parameters: %d ----' % nParams)\n",
    "\n",
    "# Define the mean-squared error loss function\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "# Initialize the custom optimizer\n",
    "optim = Optim(model.parameters(), args.optim, args.lr, args.clip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-optimization",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_cpc = 0.0\n",
    "lambda_reg = 0.0  # Regularization strength\n",
    "\n",
    "try:\n",
    "    logging.info('----Training begins----')\n",
    "    last_update = 1 \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training for one epoch\n",
    "        train_loss, train_cpc, train_corr, train_nrmse, train_mape, train_js, \\\n",
    "        att_w_snap1, att_w_snap2, edge_id_snap1, edge_id_snap2, evo_embed_train = train(\n",
    "            Data, \n",
    "            Data.train_flow[:, [0, 1]], \n",
    "            Data.train_flow[:, 2], \n",
    "            model,\n",
    "            criterion_mse,\n",
    "            lambda_reg,\n",
    "            optim, \n",
    "            epoch, \n",
    "            args.batch_size\n",
    "        )\n",
    "        \n",
    "        # Validation step\n",
    "        valid_cpc, valid_corr, valid_nrmse, valid_mape, valid_js = evaluate(\n",
    "            Data, \n",
    "            Data.valid_flow[:, [0, 1]], \n",
    "            Data.valid_flow[:, 2], \n",
    "            model, \n",
    "            path,\n",
    "            args.batch_size, \n",
    "            \"valid\"\n",
    "        )\n",
    "        \n",
    "        logging.info(\n",
    "            '| Epoch {:3d} | Time: {:5.2f}s | Train Loss: {:5.4f} | '\n",
    "            'Train CPC: {:5.4f} | Train Corr: {:5.4f} | Train NRMSE: {:5.4f} | '\n",
    "            'Train MAPE: {:5.4f} | Train JS: {:5.4f} | Valid CPC: {:5.4f} | '\n",
    "            'Valid Corr: {:5.4f} | Valid NRMSE: {:5.4f} | Valid MAPE: {:5.4f} | '\n",
    "            'Valid JS: {:5.4f}'.format(\n",
    "                epoch, (time.time() - epoch_start_time), train_loss, train_cpc, train_corr,\n",
    "                train_nrmse, train_mape, train_js, valid_cpc, valid_corr, valid_nrmse, valid_mape, valid_js\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Save model if performance improves\n",
    "        if best_valid_cpc < valid_cpc and valid_cpc < 1:\n",
    "            logging.info(\"----Epoch: {}, saving model----\".format(epoch))\n",
    "            with open(os.path.join(path_model, args.test_model_name + \"model.pkl\"), 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            with open(os.path.join(path_log, args.test_model_name + \"log.txt\"), \"a\") as file:\n",
    "                file.write(\"Epoch: {}, model saved.\\r\\n\".format(epoch))\n",
    "            best_valid_cpc = valid_cpc\n",
    "            last_update = epoch\n",
    "\n",
    "        # Early stopping condition: if no improvement in 200 epochs\n",
    "        if epoch - last_update == 200:\n",
    "            with open(os.path.join(path_model, args.test_model_name + \"model_last_epoch.pkl\"), 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.info('-' * 90)\n",
    "    logging.info('----Exiting training early----')\n",
    "    if not os.path.isfile(os.path.join(path_model, args.test_model_name + \"model_early_stop.pkl\")):\n",
    "        with open(os.path.join(path_model, args.test_model_name + \"model_early_stop.pkl\"), 'wb') as f:\n",
    "            torch.save(model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-eight",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"----Testing begins----\")\n",
    "# Try loading the best saved model\n",
    "if os.path.isfile(os.path.join(path_model, args.test_model_name + \"model.pkl\")):\n",
    "    with open(os.path.join(path_model, args.test_model_name + \"model.pkl\"), 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    test_cpc, test_corr, test_nrmse, test_mape, test_js = evaluate(\n",
    "        Data, \n",
    "        Data.test_flow[:, [0, 1]], \n",
    "        Data.test_flow[:, 2], \n",
    "        model, \n",
    "        path,\n",
    "        args.batch_size, \n",
    "        \"test\"\n",
    "    )\n",
    "    logging.info(args)\n",
    "elif os.path.isfile(os.path.join(path_model, args.test_model_name + \"model_last_epoch.pkl\")):\n",
    "    with open(os.path.join(path_model, args.test_model_name + \"model_last_epoch.pkl\"), 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    test_cpc, test_corr, test_nrmse, test_mape, test_js = evaluate(\n",
    "        Data, \n",
    "        Data.test_flow[:, [0, 1]], \n",
    "        Data.test_flow[:, 2], \n",
    "        model, \n",
    "        path,\n",
    "        args.batch_size, \n",
    "        \"test\"\n",
    "    )\n",
    "    logging.info(args)\n",
    "else:\n",
    "    with open(os.path.join(path_model, args.test_model_name + \"model_early_stop.pkl\"), 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    test_cpc, test_corr, test_nrmse, test_mape, test_js = evaluate(\n",
    "        Data, \n",
    "        Data.test_flow[:, [0, 1]], \n",
    "        Data.test_flow[:, 2], \n",
    "        model, \n",
    "        path,\n",
    "        args.batch_size, \n",
    "        \"test\"\n",
    "    )\n",
    "    logging.info(args)\n",
    "\n",
    "logging.info(\"Test Metrics -> CPC: {:5.4f} | Corr: {:5.4f} | NRMSE: {:5.4f} | MAPE: {:5.4f} | JS: {:5.4f}\".format(\n",
    "    test_cpc, test_corr, test_nrmse, test_mape, test_js\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-hospital",
   "metadata": {},
   "source": [
    "## Running `main.py` Directly\n",
    "\n",
    "If you prefer to run the entire pipeline as a standalone script (instead of stepping through this notebook), you can use the `main.py` file directly from the command line.\n",
    "\n",
    "**Usage Example:**\n",
    "\n",
    "```\n",
    "python main.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-nepal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BiSEN",
   "language": "python",
   "name": "bisen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
